{"pages":[{"title":"about","text":"一个沉迷于IT技术的机械工程师 海贼迷 相信努力和梦想一定能创造更美好的明天~","link":"/about/index.html"}],"posts":[{"title":"Python时间格式总结","text":"time模块 time模块下时间主要有三种表现形式： 时间戳(timestamp)，注意这里所说的时间戳本质上是个数字(int/float)，与Pandas中pandas._libs.tslibs.timestamps.Timestamp的timestamp是不一样的； 时间元组(struct_time); 格式化时间，格式化时间还包括自定义格式和固定格式； 以上表现形式生成和相互转换的方式： 时间戳(ts) 时间元组(st) 自定义格式(ft) 固定格式 时间戳 time.time() time.localtime(ts)time.gmtime(ts) NA time.ctime(ts) 时间元组 time.mktime(st) time.localtime()time.gmtime() time.strptime(fmt, st) time.asctime(st) 自定义格式 NA NA time.strftime(fmt) NA 固定格式 NA NA NA NA datetime.date类 date类，表示日期的类： 创建方法：datetime.date(year, month, day) 主要属性：year，month，day 主要方法： date.timetuple(date) —— 返回日期的时间元组； date.replace(year, month, day) —— 替换date对象中的year/month/day，并返回替换后的date对象； date.weekday(date) —— 返回日期对应的星期，星期一返回0，星期二返回1，以此类推； date.isoweekday(date) —— 返回日期对应的星期，星期一返回1，星期二返回2，以此类推； date.iscalendar(date) —— 返回日期对应的元组 ； date.isoformat(date) —— 返回日期对应格式的字符串； date.fromtimestamp(timestamp) —— 根据给定的时间戳返回一个date对象； date.today() —— 返回当地日期的date对象，注意这里返回的不是timestamp对象； datetime.time类 time类，表示时间的类： 创建方法：datetime.time(hour, minute, second, microsecond, tzinfo) 主要属性：hour, minute, second, microsecond, tzinfo 主要方法： time.replace(hour, minute, second, microsecond, tzinfo) —— 替换time对象中的时分秒微秒和时区，并返回替换后的time对象； time.isofomr(time)，根据time对象返回 hour:minute:second格式的时间； time.strftime(fmt)，根据指定的格式，返回time对象的自定义格式化时间的字符串； datetime.datetime类 datetime类，饱含date和time的所有信息。 主要属性：year、month、day、hour、minute、second、microsecond、tzinfo； 主要方法： datetime.date()/time()，获取date/time对象； datetime.today()，获取表示当前本地时间的datetime对象； datetime.now(tz)，获取表示当前本地时间的datetime对象，如果提供时区参数tz，则获取指定时区的datetime对象； datetime.utcnow()，获取表示当前utc时间的datetime对象； datetime.fromtimestamp(timestamp, tz)，根据时间戳创建一个datetime对象，如果指定了tz参数，则返回指定时区的对象； datetime.utcfromtimestamp(timestamp)，根据时间戳创建一个datetime对象； datetime.combine(date, time)，根据date和time对象创建一个datetime对象； datetime.strptime(date_string, format)，将格式字符串转换为datetime对象； datetime.replace(year, month, day, hour, minute, second, microsecond, tzinfo)，替代datetime中的年月日时分秒微秒和时区，并返回替换后的datetime对象； datetime.timetuple(datetime)，根据datetime对象，返回对应的时间元组； datetime.utctimetuple(datetime)，根据datetime对象，返回对应的utc时间的时间元组； datetime.weekday(datetime)，返回datetime对象所对应的星期，星期一对应于0，星期二对应1，以此类推； datetime.isoweekday(datetime)，返回datetime对象所对应的星期，星期一对应于1，星期二对应2，以此类推； datetime.isocalendar (datetime)，返回datetime对象说对应的iso日历； datetime.isoformat(datetime)，返回datetime对象所对应的iso格式的时间； datetime.ctime()，返回datetime对象所对应的c格式时间的字符串； datetime.strftime(format)，返回datetime对象所对应的指定格式时间的字符串； 参考文献：https://www.cnblogs.com/tkqasn/p/6001134.html https://my.oschina.net/whp/blog/130710","link":"/2020/09/20/Python%E6%97%B6%E9%97%B4%E6%A0%BC%E5%BC%8F%E6%80%BB%E7%BB%93/"},{"title":"pandas.dataframe切片总结","text":"df[‘column_name’] ，df[row_start_index, row_end_index] 可以通过df[[‘column_name1’, ‘column_name2’]]的方式来指定想选择的单个或多个列； 可以通过df[df.index == ‘index’]来选择指定的单个行； 可以通过行的序号来指定想选择的单个或多个行； 不能通过df[‘index’]的方式来选择指定的行； 不能通过列的序号来指定想选择的列； loc函数 可以通过df.loc[[‘index_name1’,’index_name2’], [‘column_name1’, ‘column_name2’]]来指定想选择的单个或多个行和列; 可以通过行的序号来指定想选择的单个或多个行； 可以通过df.loc[df.index == ‘index’]来选择指定的单个行； iloc函数 可以通过df.iloc[df.index == ‘index’]来选择指定的单个行； 可以通过行列的序号来指定想要选择的行和列；","link":"/2020/09/20/pandas-dataframe%E5%88%87%E7%89%87%E6%80%BB%E7%BB%93/"},{"title":"Spark与TDengine连接总结","text":"TDengine是一款高性能的时序数据库，最近在研究如何在Spark中读取和写入数据，这里记录一下最近的成果。 版本信息： Spark版本：2.4.0，yarn集群模式 Python版本：3.7.9 Scala版本：2.11.12 TDengine版本：2.6，商业版 TAOS-JDBC版本：2.0.42 1. 官方JDBC使用TDengine官方提供了JDBC，Spark读取和写入均可以直接使用 1.1 依赖问题 需要引用taos-jdbcdriver，版本上强烈推荐2.0.42，其他版本会有各种问题(针对2.x系列，3.x系列的没用过)； 12345&lt;dependency&gt; &lt;groupId&gt;com.taosdata.jdbc&lt;/groupId&gt; &lt;artifactId&gt;taos-jdbcdriver&lt;/artifactId&gt; &lt;version&gt;2.0.42&lt;/version&gt; &lt;/dependency&gt; taos-jdbcdriver的依赖与spark集群可能存在冲突(版本不同)，需要手动指定依赖，方法为： 下载正确的依赖，我用的是guava-30.1.1-jre.jar和failureaccess-1.0.1.jar 将jar上传hadoop； 在提交spark任务的时候，手动制定依赖： 123--conf spark.driver.extraClassPath=guava-30.1.1-jre.jar:failureaccess-1.0.1.jar \\--conf spark.executor.extraClassPath=guava-30.1.1-jre.jar:failureaccess-1.0.1.jar --jars hadoop-path-to-jar/guava-30.1.1-jre.jar,hadoop-path-to-jar/failureaccess-1.0.1.jar 对于pyspark程序，最好引用taos-jdbcdriver-2.0.42-dist.jar，补足依赖 123--conf spark.driver.extraClassPath=guava-30.1.1-jre.jar:failureaccess-1.0.1.jar:taos-jdbcdriver-2.0.42-dist.jar --conf spark.executor.extraClassPath=guava-30.1.1-jre.jar:failureaccess-1.0.1.jar:taos-jdbcdriver-2.0.42-dist.jar --jars hadoop-path-to-jar/guava-30.1.1-jre.jar,hadoop-path-to-jar/failureaccess-1.0.1.jar,hadoop-path-to-jar/taos-jdbcdriver-2.0.42-dist.jar 1.2 读取Spark通过TDBC读取TDengine数据和其他数据库并没有什么本质区别，可以通过以下方法来读取： 读取整张表 123456info = spark.read\\ .format(&quot;jdbc&quot;)\\ .option(&quot;driver&quot;, &quot;com.taosdata.jdbc.rs.RestfulDriver&quot;)\\ .option(&quot;url&quot;, &quot;jdbc:TAOS-RS://ip:port/db?user=user&amp;password=password&quot;)\\ .option(&quot;dbtable&quot;, &quot;test.test&quot;)\\ .load() 读取部分内容 123456info = spark.read\\ .format(&quot;jdbc&quot;)\\ .option(&quot;driver&quot;, &quot;com.taosdata.jdbc.rs.RestfulDriver&quot;)\\ .option(&quot;url&quot;, &quot;jdbc:TAOS-RS://ip:port/db?user=user&amp;password=password&quot;)\\ .option(&quot;query&quot;, &quot;select * from test.test&quot;)\\ .load() 如果读取的数量较多，可能会遇到timeout错误，可以设置httpSocketTimeout，单位是ms，默认值为5000 1.3 存储可以使用下面的方法向TDengine保存数据 12345678toHbaseSave .write .format(&quot;jdbc&quot;) .option(&quot;url&quot;, &quot;jdbc:TAOS-RS://ip:port/db?user=user&amp;password=password&quot;) .option(&quot;driver&quot;, &quot;com.taosdata.jdbc.rs.RestfulDriver&quot;) .mode(SaveMode.Append) .option(&quot;dbtable&quot;, &quot;test.test&quot;) .save() 2. 使用自定义数据源写入数据对于spark来说，直接使用官方的jdbc进行连接只能实现向普通表中写入数据，无法利用TDengine超级表的特性，为了能向超级表中写入数据，需要自定义数据源。 由于只需要写入特性，因此下面的内容中只实现了写入的逻辑，没有实现读取的逻辑，使用scala编写。 继承DataSourceV2和WriteSupport 并重写createWriter方法，创建自定义的数据源； 12345678910111213141516171819class TDSourceV2 extends DataSourceV2 with WriteSupport with Serializable { override def createWriter( jobId: String, structType: StructType, saveMode: SaveMode, dataSourceOptions: DataSourceOptions): Optional[DataSourceWriter] = { Optional.of(new TDSourceWriter( // url dataSourceOptions.get(&quot;url&quot;).get(), // 用户名 dataSourceOptions.get(&quot;user&quot;).get(), // 密码 dataSourceOptions.get(&quot;password&quot;).get(), // 数据库名称 dataSourceOptions.get(&quot;db&quot;).get(), // 超级表名称 dataSourceOptions.get(&quot;stable&quot;).get())) } 继承 DataSourceWriter 重写 createWriterFactory 方法并返回自定义的 DataWriterFactory，重写 commit 方法，用来提交整个事务, 重写 abort 方法，用来做事务回滚； 123456789101112131415class TDSourceWriter( url: String, user: String, password: String, db: String, stable: String) extends DataSourceWriter with Serializable{ override def createWriterFactory(): DataWriterFactory[InternalRow] = { new TDWriterFactory(url, user, password, db, stable) } // 2.6版本的TDengine不支持事务 override def commit(writerCommitMessages: Array[WriterCommitMessage]): Unit = Unit // 2.6版本的TDengine不支持事务 override def abort(writerCommitMessages: Array[WriterCommitMessage]): Unit = Unit } 继承 DataWriterFactory, 重写 createDataWriter方法返回自定义的 DataWriter； 12345678910111213class TDWriterFactory( url: String, user: String, password: String, db: String, stable: String) extends DataWriterFactory[InternalRow] with Serializable { override def createDataWriter( partitionId: Int, taskId: Long, epochId: Long): DataWriter[InternalRow] = { new TDDataWriter(url, user, password, db, stable) } } 继承 DataWriter 重写 write 方法实现具体的写入数据库逻辑，重写 commit 方法用来提交事务，重写 abort 方法用来做事务回滚 ； 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657class TDDataWriter( url: String, user: String, password: String, db: String, stable: String) extends DataWriter[InternalRow] with Serializable { private val logger = LoggerFactory.getLogger(this.getClass) private var conn: Connection = null private var stmt: Statement = null override def write(record: InternalRow): Unit = { // 在这里编写将数据插入数据库的逻辑 Class.forName(&quot;com.taosdata.jdbc.rs.RestfulDriver&quot;) val jdbcUrl = s&quot;jdbc:TAOS-RS://$url/$db?user=$user&amp;password=$password&quot; // 可以通过record.getxxx获取对应列的内容 // 如table_name = record.getString(0) val variable1 = record.getString(0) val variable2 = record.getInt(1) val variable3 = record.getFloat(2) // 构建插入数据库的sql语句 // 请注意，只有原生连接方式才支持参数绑定的插入方式， // 由于要在spark集群上跑，这里用的是Rest的连接方式， // 所以只能手动构建插入数据库的sql语句， // 关于原生连接、Rest连接和参数绑定插入，请见官方文档 val sql = &quot;insert to ...&quot; logger.info(sql) conn = DriverManager.getConnection(jdbcUrl) stmt = conn.createStatement() stmt.execute(s&quot;use $db&quot;) try{ stmt.execute(sql) conn.commit() }catch { case e: Exception =&gt; e.printStackTrace() }finally { conn.close() } } // Tdengine2.6版本不支持事务，因此此处并无实际需要做的事情 // 创建WriterCommitMessage类，绝对不能传null，网上有些代码是错误的 object WriteSucceeded extends WriterCommitMessage override def commit(): WriterCommitMessage = WriteSucceeded // Tdengine2.6版本不支持事务，因此此处并无实际需要做的事情 override def abort(): Unit = Unit } 调用，在程序中指定自定义的datasource并传入参数 123456789101112131415toTdSave .write .format(&quot;org.example.TDSourceV2&quot;) .mode(SaveMode.Append) // url .option(&quot;url&quot;, url) // 数据库名 .option(&quot;db&quot;, db) // 用户名 .option(&quot;user&quot;, user) // 超级表名 .option(&quot;stable&quot;, stable) // 密码 .option(&quot;password&quot;, password) .save() 参考文献： 暑期2021项目经验分享：实现Spark对接openGauss Spark DataSource V1 &amp; V2 API 一文理解 Spark SQL DataSource V2 学习入门 + 代码模板 Tdengine v2.6 官方文档","link":"/2023/02/23/Spark%E4%B8%8ETDengine%E9%93%BE%E6%8E%A5%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"Pandas","slug":"Pandas","link":"/tags/Pandas/"},{"name":"Spark","slug":"Spark","link":"/tags/Spark/"},{"name":"TDengine","slug":"TDengine","link":"/tags/TDengine/"}],"categories":[]}